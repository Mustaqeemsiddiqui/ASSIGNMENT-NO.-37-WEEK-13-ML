{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b82b87fa-04ff-40e9-818f-b249f5f87837",
   "metadata": {},
   "source": [
    "**Q1. What is the Filter method in feature selection, and how does it work?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c3b614-a282-4bd0-8dd2-3afffd3aafe7",
   "metadata": {},
   "source": [
    "**ANSWER:---**\n",
    "\n",
    "The Filter method in feature selection is a technique used to select important features (variables) in a dataset based on certain statistical criteria, without involving any machine learning algorithms. The main goal of this method is to remove irrelevant or less important features before the modeling process, thereby improving the performance and reducing the complexity of the model.\n",
    "\n",
    "### How the Filter Method Works:\n",
    "\n",
    "1. **Statistical Criteria**: The filter method relies on statistical measures to evaluate the importance of each feature with respect to the target variable. Common statistical criteria include correlation coefficients, chi-square tests, mutual information, variance thresholds, and ANOVA F-tests.\n",
    "\n",
    "2. **Independence from Models**: Unlike wrapper and embedded methods, filter methods do not involve training models. They are independent of any machine learning algorithm, which makes them computationally efficient and faster.\n",
    "\n",
    "3. **Ranking and Selection**: Features are ranked based on their scores from the statistical tests. A threshold is set (either manually or automatically) to select the top-ranked features. For example, if the correlation between a feature and the target variable is high, that feature might be considered important.\n",
    "\n",
    "### Common Statistical Measures Used in Filter Methods:\n",
    "\n",
    "- **Correlation Coefficient (Pearson, Spearman)**: Measures the linear or rank-based relationship between features and the target variable. Features with high correlation are selected.\n",
    "  \n",
    "- **Chi-Square Test**: Used for categorical features to assess the independence between a feature and the target variable. Features with low p-values are considered important.\n",
    "\n",
    "- **Mutual Information**: Measures the amount of information shared between a feature and the target variable. Higher mutual information indicates higher relevance.\n",
    "\n",
    "- **Variance Threshold**: Removes features with low variance, assuming they have little information to contribute to the model.\n",
    "\n",
    "- **ANOVA F-test**: Compares the means of different groups for categorical features to find significant differences that might indicate importance.\n",
    "\n",
    "### Advantages and Disadvantages of Filter Methods:\n",
    "\n",
    "**Advantages**:\n",
    "- **Simplicity and Speed**: Since no model training is involved, filter methods are computationally efficient and faster than wrapper and embedded methods.\n",
    "- **Model Independence**: They do not depend on a specific learning algorithm, making them more generalizable.\n",
    "- **Scalability**: Suitable for large datasets as they are less computationally intensive.\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Ignoring Feature Interactions**: Filter methods evaluate features individually and may miss interactions between features that could be important.\n",
    "- **Potential for Overlooking Non-linear Relationships**: Simple statistical measures might not capture complex relationships between features and the target variable.\n",
    "\n",
    "### Example Workflow of the Filter Method:\n",
    "\n",
    "1. **Calculate Statistical Measures**: Compute the chosen statistical measure (e.g., correlation) between each feature and the target variable.\n",
    "2. **Rank Features**: Rank the features based on the computed scores.\n",
    "3. **Set a Threshold**: Determine a threshold for selection (e.g., top 10 features or features with a correlation above 0.5).\n",
    "4. **Select Features**: Select the top-ranked features that meet the threshold criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3f83a8-edfc-4d08-8d47-43217ef6fb95",
   "metadata": {},
   "source": [
    "**Q2. How does the Wrapper method differ from the Filter method in feature selection?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa9f53a-6e8d-4ae3-a4b0-ec064345c8f4",
   "metadata": {},
   "source": [
    "**ANSWER:----**\n",
    "\n",
    "The Wrapper method and the Filter method are both techniques used for feature selection, but they differ significantly in their approaches, processes, and the way they evaluate features. Here’s a detailed comparison of the two methods:\n",
    "\n",
    "### Wrapper Method:\n",
    "\n",
    "1. **Model-Based Evaluation**: The Wrapper method evaluates the importance of features by using a predictive model. It assesses different subsets of features by training and testing a model on each subset and selecting the subset that produces the best performance.\n",
    "\n",
    "2. **Iterative Search**: Wrapper methods typically involve an iterative search process, such as forward selection, backward elimination, or recursive feature elimination. These methods add or remove features one at a time, evaluating the model performance at each step.\n",
    "\n",
    "3. **Performance Metric**: The selection process is guided by a performance metric (e.g., accuracy, F1-score, AUC) obtained from the model. Features are selected based on how well they improve this performance metric.\n",
    "\n",
    "4. **Computationally Intensive**: Since wrapper methods involve training and testing models multiple times on different subsets of features, they are computationally more expensive and time-consuming than filter methods.\n",
    "\n",
    "5. **Capturing Feature Interactions**: Wrapper methods can capture interactions between features because they evaluate the combined effect of features on the model’s performance.\n",
    "\n",
    "### Filter Method:\n",
    "\n",
    "1. **Statistical Criteria**: The Filter method evaluates the importance of features based on statistical measures or criteria such as correlation coefficients, chi-square tests, mutual information, variance thresholds, or ANOVA F-tests. These measures assess the relationship between each feature and the target variable independently of any model.\n",
    "\n",
    "2. **Single Step Process**: Filter methods usually involve a single-step process where features are ranked based on the chosen statistical measure, and the top-ranked features are selected.\n",
    "\n",
    "3. **No Model Training**: Filter methods do not involve training and testing predictive models, making them computationally efficient and faster compared to wrapper methods.\n",
    "\n",
    "4. **Ignoring Feature Interactions**: Since filter methods evaluate features independently, they may miss important interactions between features.\n",
    "\n",
    "5. **Simplicity and Scalability**: Filter methods are simpler and more scalable, especially suitable for large datasets due to their lower computational requirements.\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "1. **Evaluation Basis**:\n",
    "   - **Wrapper Method**: Evaluates feature subsets based on model performance.\n",
    "   - **Filter Method**: Evaluates individual features based on statistical measures.\n",
    "\n",
    "2. **Process**:\n",
    "   - **Wrapper Method**: Iterative and involves model training and testing for each subset of features.\n",
    "   - **Filter Method**: Single-step process without involving model training.\n",
    "\n",
    "3. **Computational Complexity**:\n",
    "   - **Wrapper Method**: More computationally intensive due to repeated model training and testing.\n",
    "   - **Filter Method**: Less computationally intensive and faster.\n",
    "\n",
    "4. **Feature Interactions**:\n",
    "   - **Wrapper Method**: Can capture interactions between features.\n",
    "   - **Filter Method**: Ignores interactions between features.\n",
    "\n",
    "5. **Applicability**:\n",
    "   - **Wrapper Method**: More suitable for smaller datasets where computational cost is not a major concern.\n",
    "   - **Filter Method**: More suitable for large datasets due to its efficiency and speed.\n",
    "\n",
    "### Example of Each Method:\n",
    "\n",
    "- **Wrapper Method**:\n",
    "  - **Forward Selection**: Start with no features, add features one by one, and select the subset that maximizes the model’s performance.\n",
    "  - **Backward Elimination**: Start with all features, remove features one by one, and select the subset that maintains or improves model performance.\n",
    "\n",
    "- **Filter Method**:\n",
    "  - **Correlation Coefficient**: Calculate the correlation between each feature and the target variable, rank features by correlation, and select the top-ranked features.\n",
    "  - **Chi-Square Test**: Perform a chi-square test for independence between each feature and the target variable, rank features by their p-values, and select the most significant features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffc1c60-70ed-4e73-a15d-3eca69590361",
   "metadata": {},
   "source": [
    "**Q3. What are some common techniques used in Embedded feature selection methods?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a9021e-a873-4f46-a3e1-35c595ceae23",
   "metadata": {},
   "source": [
    "**ANSWER:----**\n",
    "\n",
    "Embedded feature selection methods integrate the process of feature selection with the training process of a predictive model. These methods select features while the model is being built, often leveraging the model's own mechanisms for identifying important features. Here are some common techniques used in embedded feature selection methods:\n",
    "\n",
    "### Common Embedded Feature Selection Techniques:\n",
    "\n",
    "1. **Regularization Methods**:\n",
    "   - **Lasso (L1 Regularization)**: Adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function. This can shrink some coefficients to zero, effectively performing feature selection.\n",
    "   - **Ridge (L2 Regularization)**: Adds a penalty equal to the square of the magnitude of coefficients. While it does not perform feature selection directly, it can be used in conjunction with other methods.\n",
    "   - **Elastic Net**: Combines L1 and L2 regularization, promoting both sparsity (like Lasso) and grouping of correlated features.\n",
    "\n",
    "2. **Tree-Based Methods**:\n",
    "   - **Decision Trees**: Inherently perform feature selection by splitting nodes based on the most informative features. Features used in splits are considered important.\n",
    "   - **Random Forests**: An ensemble of decision trees, where the importance of a feature is determined by averaging the importance of that feature across all trees in the forest.\n",
    "   - **Gradient Boosting Machines (GBMs)**: Similar to random forests but builds trees sequentially. Feature importance is determined based on the contribution of each feature to the reduction of the loss function.\n",
    "\n",
    "3. **Regularized Linear Models**:\n",
    "   - **Logistic Regression with L1 Regularization**: For classification tasks, logistic regression with L1 regularization can be used to perform feature selection by shrinking some coefficients to zero.\n",
    "   - **Support Vector Machines (SVM) with L1 Penalty**: For linear SVMs, using an L1 penalty can lead to sparse solutions where some feature weights are zero.\n",
    "\n",
    "4. **Others**:\n",
    "   - **Least Absolute Shrinkage and Selection Operator (LASSO)**: A regression analysis method that performs both feature selection and regularization to enhance the prediction accuracy and interpretability of the statistical model it produces.\n",
    "   - **Embedded Methods in Neural Networks**: Techniques like dropout and L1/L2 regularization can be used in neural networks to encourage sparsity and reduce overfitting, indirectly performing feature selection.\n",
    "\n",
    "### Key Characteristics of Embedded Methods:\n",
    "\n",
    "- **Integrated Process**: Feature selection occurs during the model training process, often resulting in more efficient and effective selection.\n",
    "- **Algorithm-Specific**: These methods are usually tied to specific types of models or algorithms, such as linear models or tree-based methods.\n",
    "- **Automatic Selection**: Embedded methods automatically select features based on the criteria defined by the model's learning process.\n",
    "\n",
    "### Advantages and Disadvantages:\n",
    "\n",
    "**Advantages**:\n",
    "- **Efficiency**: Since feature selection is integrated into the training process, it can be computationally efficient compared to wrapper methods.\n",
    "- **Better Generalization**: By selecting features based on model performance, embedded methods often result in models that generalize better to unseen data.\n",
    "- **Model Interpretability**: Many embedded methods, especially those involving regularization, produce models that are easier to interpret due to the reduced number of features.\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Algorithm Dependency**: These methods are specific to the algorithm used and may not be easily transferable to other types of models.\n",
    "- **Complexity**: The integration of feature selection into the model training process can sometimes add complexity to the model-building process.\n",
    "\n",
    "### Examples of Application:\n",
    "\n",
    "- **Lasso Regression for Sparse Models**: Used in high-dimensional datasets where feature selection and prediction are both needed.\n",
    "- **Random Forests for Variable Importance**: Often used in problems where understanding the importance of variables is crucial, such as in medical or financial applications.\n",
    "- **Gradient Boosting for Robust Prediction**: Used in competitive machine learning and scenarios requiring high predictive accuracy, with feature importance providing insights into the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9bd801-1e47-4c3b-9f5c-ec10518bfcb0",
   "metadata": {},
   "source": [
    "**Q4. What are some drawbacks of using the Filter method for feature selection?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b235d85-4aaa-459d-aaa6-8019f0b397c4",
   "metadata": {},
   "source": [
    "**ANSWER:-----**\n",
    "\n",
    "The Filter method for feature selection, while simple and computationally efficient, does have several drawbacks:\n",
    "\n",
    "1. **Ignoring Feature Interactions**: Filter methods evaluate each feature independently without considering the interactions between features. This can lead to the exclusion of features that, while not highly informative on their own, might be important in combination with other features.\n",
    "\n",
    "2. **Oversimplified Assumptions**: The statistical measures used in filter methods (e.g., correlation, mutual information) often assume linear relationships between features and the target variable. This can result in the omission of features that have non-linear but significant relationships with the target.\n",
    "\n",
    "3. **Potential for Overlooking Non-linear Relationships**: Simple statistical criteria may not capture complex, non-linear relationships between features and the target variable, potentially overlooking important features.\n",
    "\n",
    "4. **Relevance to Specific Models**: Filter methods do not account for the specific learning algorithm to be used. Features selected based on general statistical criteria may not be the most useful for a particular model or algorithm.\n",
    "\n",
    "5. **Threshold Sensitivity**: The choice of threshold for selecting features can be somewhat arbitrary and may significantly impact the results. Setting the threshold too high may exclude useful features, while setting it too low may include irrelevant features.\n",
    "\n",
    "6. **Stability Issues**: Filter methods might be unstable in the presence of noisy data or when dealing with high-dimensional datasets. The selected features can vary significantly with slight changes in the data.\n",
    "\n",
    "7. **Scalability Concerns**: While filter methods are generally more scalable than wrapper or embedded methods, they can still struggle with very large datasets if the chosen statistical test is computationally intensive.\n",
    "\n",
    "8. **No Consideration of Model Performance**: Filter methods do not directly evaluate the impact of selected features on model performance. As a result, the selected features may not necessarily lead to the best-performing model.\n",
    "\n",
    "### Example Scenario Highlighting Drawbacks:\n",
    "\n",
    "Consider a dataset with features that interact in complex ways. A feature might not show a strong individual correlation with the target variable but could be crucial when combined with other features. A filter method that looks at each feature independently would likely miss such interactions, potentially leading to suboptimal feature selection.\n",
    "\n",
    "### Comparison with Other Methods:\n",
    "\n",
    "- **Wrapper Methods**: While more computationally intensive, wrapper methods evaluate feature subsets based on model performance, capturing interactions between features and tailoring the selection to the specific algorithm.\n",
    "- **Embedded Methods**: These integrate feature selection within the model training process, often leveraging regularization techniques to select features based on their contribution to model performance, accounting for feature interactions and algorithm-specific needs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f92864-dc92-49f1-93df-21c00adb41f8",
   "metadata": {},
   "source": [
    "**Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature\n",
    "selection?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45cb501-81e8-490f-ac70-6542d3106e72",
   "metadata": {},
   "source": [
    "**ANSWER:-----**\n",
    "\n",
    "Choosing between the Filter method and the Wrapper method for feature selection depends on various factors such as dataset size, computational resources, model requirements, and specific goals of the analysis. Here are situations where the Filter method would be preferred over the Wrapper method:\n",
    "\n",
    "### Situations Favoring the Filter Method:\n",
    "\n",
    "1. **Large Datasets**:\n",
    "   - **High Dimensionality**: When dealing with datasets that have a large number of features, the Filter method is computationally more efficient and scalable, making it suitable for high-dimensional data.\n",
    "   - **Big Data**: In scenarios where the volume of data is very large, the Filter method's computational efficiency is advantageous.\n",
    "\n",
    "2. **Computational Constraints**:\n",
    "   - **Limited Resources**: If there are constraints on computational power and time, the Filter method is preferred as it is faster and less resource-intensive compared to the Wrapper method.\n",
    "\n",
    "3. **Initial Feature Reduction**:\n",
    "   - **Preprocessing Step**: The Filter method is useful as an initial step to quickly eliminate irrelevant or redundant features before applying more computationally intensive methods like Wrapper or Embedded methods.\n",
    "\n",
    "4. **Quick Insights and Interpretability**:\n",
    "   - **Simple and Transparent Criteria**: The Filter method provides a straightforward way to assess feature importance based on clear statistical criteria, making it easier to interpret and explain to stakeholders.\n",
    "\n",
    "5. **Baseline Models**:\n",
    "   - **Initial Model Building**: For building baseline models quickly and establishing a benchmark, the Filter method helps in reducing the feature set without involving complex model training.\n",
    "\n",
    "6. **Irrelevant Feature Removal**:\n",
    "   - **Obvious Irrelevance**: In cases where certain features are clearly irrelevant or have no meaningful relationship with the target variable, Filter methods can effectively remove such features early in the analysis.\n",
    "\n",
    "7. **Independent Feature Evaluation**:\n",
    "   - **No Need for Interaction Analysis**: If the problem does not require considering interactions between features or if interactions are known to be minimal, the Filter method is suitable.\n",
    "\n",
    "### Example Scenarios:\n",
    "\n",
    "1. **Text Mining**:\n",
    "   - When working with text data involving thousands of features (e.g., words or n-grams), using Filter methods like term frequency-inverse document frequency (TF-IDF) or chi-square tests to reduce dimensionality before applying more sophisticated models.\n",
    "\n",
    "2. **Preliminary Data Analysis**:\n",
    "   - In exploratory data analysis, using Filter methods to gain initial insights into which features might be important based on statistical measures, before committing to more resource-intensive methods.\n",
    "\n",
    "3. **Genomic Data**:\n",
    "   - In bioinformatics, where datasets can have tens of thousands of genetic markers, Filter methods can quickly reduce the number of features to a manageable size before applying detailed modeling techniques.\n",
    "\n",
    "4. **Real-time Applications**:\n",
    "   - For applications requiring real-time or near-real-time processing, where computational efficiency is critical, the Filter method provides a quick way to reduce the feature set.\n",
    "\n",
    "### Contrast with Wrapper Methods:\n",
    "\n",
    "- **Wrapper Methods**: While more thorough in evaluating feature subsets and their interactions, Wrapper methods are computationally intensive as they involve training and testing models repeatedly for different feature combinations.\n",
    "- **Filter Methods**: They are faster and less resource-intensive, suitable for initial feature reduction, large datasets, and situations with limited computational resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4ac8cd-5630-4848-87a9-ba67dc40839c",
   "metadata": {},
   "source": [
    "**Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn.\n",
    "You are unsure of which features to include in the model because the dataset contains several different\n",
    "ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739ddefc-29a2-46ce-9682-875a76b7e156",
   "metadata": {},
   "source": [
    "**ANSWER:-----**\n",
    "\n",
    "To select the most pertinent features for a predictive model for customer churn using the Filter method, you can follow these steps:\n",
    "\n",
    "### Step-by-Step Process:\n",
    "\n",
    "1. **Understand the Data**:\n",
    "   - Familiarize yourself with the dataset, including the types of features (e.g., numerical, categorical), their distributions, and their potential relevance to customer churn.\n",
    "\n",
    "2. **Data Preprocessing**:\n",
    "   - **Handle Missing Values**: Address missing data through imputation or removal.\n",
    "   - **Encode Categorical Variables**: Convert categorical variables into numerical values using techniques like one-hot encoding or label encoding.\n",
    "   - **Normalize/Scale Data**: Standardize numerical features to ensure they are on a similar scale.\n",
    "\n",
    "3. **Select Statistical Criteria**:\n",
    "   Choose appropriate statistical measures to evaluate the relationship between each feature and the target variable (churn). Common criteria include:\n",
    "\n",
    "   - **Correlation Coefficient**: For numerical features, calculate the Pearson or Spearman correlation between each feature and the churn variable.\n",
    "   - **Chi-Square Test**: For categorical features, perform chi-square tests to assess the independence between each feature and churn.\n",
    "   - **Mutual Information**: Measure the mutual information between each feature and the churn variable to understand the amount of shared information.\n",
    "   - **Variance Threshold**: Remove features with low variance, as they provide little information.\n",
    "\n",
    "4. **Compute Statistical Scores**:\n",
    "   - Calculate the chosen statistical measure for each feature in relation to the churn variable.\n",
    "   - For numerical features, use correlation coefficients or mutual information.\n",
    "   - For categorical features, use chi-square tests or mutual information.\n",
    "\n",
    "5. **Rank Features**:\n",
    "   - Rank the features based on their computed statistical scores. Higher scores indicate a stronger relationship with the churn variable.\n",
    "\n",
    "6. **Set a Selection Threshold**:\n",
    "   - Determine a threshold for selecting features. This could be based on a predefined number of top features, a specific score cutoff, or using cross-validation to find the optimal threshold.\n",
    "   - For example, you might choose the top 10 features with the highest correlation coefficients or chi-square scores.\n",
    "\n",
    "7. **Select Features**:\n",
    "   - Select the features that meet the threshold criteria. These are the features deemed most pertinent for predicting customer churn.\n",
    "\n",
    "8. **Evaluate and Iterate**:\n",
    "   - Use the selected features to build an initial predictive model.\n",
    "   - Evaluate the model’s performance using appropriate metrics (e.g., accuracy, precision, recall, AUC-ROC).\n",
    "   - If performance is not satisfactory, consider adjusting the threshold, trying different statistical measures, or incorporating additional domain knowledge to refine the feature selection process.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Suppose your dataset includes features such as customer age, tenure, monthly charges, contract type, payment method, and usage patterns. Here's how you might apply the Filter method:\n",
    "\n",
    "1. **Preprocessing**:\n",
    "   - Impute missing values, encode categorical variables (e.g., contract type, payment method), and scale numerical features (e.g., monthly charges).\n",
    "\n",
    "2. **Select Statistical Criteria**:\n",
    "   - For numerical features (age, tenure, monthly charges), use the Pearson correlation coefficient.\n",
    "   - For categorical features (contract type, payment method), use chi-square tests.\n",
    "\n",
    "3. **Compute Scores**:\n",
    "   - Calculate the Pearson correlation between numerical features and churn.\n",
    "   - Perform chi-square tests for categorical features against the churn variable.\n",
    "\n",
    "4. **Rank Features**:\n",
    "   - Rank numerical features based on their correlation coefficients with churn.\n",
    "   - Rank categorical features based on their chi-square test p-values.\n",
    "\n",
    "5. **Set a Threshold and Select Features**:\n",
    "   - Select the top-ranked features, such as those with a correlation coefficient above 0.2 or chi-square p-value below 0.05.\n",
    "\n",
    "6. **Evaluate and Iterate**:\n",
    "   - Build a predictive model using the selected features.\n",
    "   - Evaluate model performance and iterate on feature selection if necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf0788fa-04e3-4d0b-872d-433856e38421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CustomerID  Age  Tenure  MonthlyCharges        Contract     PaymentMethod  \\\n",
      "0           1   23       1           29.85  Month-to-month  Electronic check   \n",
      "1           2   45       2           56.95        One year      Mailed check   \n",
      "2           3   56       8           53.85        Two year     Bank transfer   \n",
      "3           4   25       1           42.30  Month-to-month  Electronic check   \n",
      "4           5   39       5           70.70        One year     Bank transfer   \n",
      "\n",
      "   Churn  \n",
      "0      1  \n",
      "1      0  \n",
      "2      0  \n",
      "3      1  \n",
      "4      0  \n",
      "Selected Features: ['MonthlyCharges', 'Contract', 'Age', 'Tenure', 'PaymentMethod']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69/2666484648.py:22: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  data.fillna(data.mean(), inplace=True)  # Example imputation\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import chi2, mutual_info_classif\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Create a sample dataset\n",
    "data = pd.DataFrame({\n",
    "    'CustomerID': range(1, 11),\n",
    "    'Age': [23, 45, 56, 25, 39, 33, 46, 52, 40, 29],\n",
    "    'Tenure': [1, 2, 8, 1, 5, 3, 9, 10, 4, 2],\n",
    "    'MonthlyCharges': [29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89.10, 29.75, 49.95, 18.95],\n",
    "    'Contract': ['Month-to-month', 'One year', 'Two year', 'Month-to-month', 'One year', 'One year', 'Two year', 'Month-to-month', 'One year', 'Month-to-month'],\n",
    "    'PaymentMethod': ['Electronic check', 'Mailed check', 'Bank transfer', 'Electronic check', 'Bank transfer', 'Credit card', 'Electronic check', 'Mailed check', 'Credit card', 'Electronic check'],\n",
    "    'Churn': [1, 0, 0, 1, 0, 0, 0, 1, 0, 1]\n",
    "})\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Preprocess data\n",
    "data.fillna(data.mean(), inplace=True)  # Example imputation\n",
    "categorical_features = ['Contract', 'PaymentMethod']\n",
    "numerical_features = ['Age', 'Tenure', 'MonthlyCharges']\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    data[feature] = le.fit_transform(data[feature])\n",
    "    label_encoders[feature] = le\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "data[numerical_features] = scaler.fit_transform(data[numerical_features])\n",
    "\n",
    "# Calculate statistical scores\n",
    "correlation_scores = {feature: pearsonr(data[feature], data['Churn'])[0] for feature in numerical_features}\n",
    "chi2_scores, _ = chi2(data[categorical_features], data['Churn'])\n",
    "mutual_info_scores = mutual_info_classif(data[numerical_features + categorical_features], data['Churn'])\n",
    "\n",
    "# Rank features\n",
    "sorted_correlation_scores = sorted(correlation_scores.items(), key=lambda item: abs(item[1]), reverse=True)\n",
    "sorted_chi2_scores = sorted(zip(categorical_features, chi2_scores), key=lambda item: item[1], reverse=True)\n",
    "sorted_mutual_info_scores = sorted(zip(numerical_features + categorical_features, mutual_info_scores), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Select top features based on threshold\n",
    "top_features = [feature for feature, score in sorted_correlation_scores[:3]] + \\\n",
    "               [feature for feature, score in sorted_chi2_scores[:3]] + \\\n",
    "               [feature for feature, score in sorted_mutual_info_scores[:3]]\n",
    "\n",
    "# Remove duplicates\n",
    "top_features = list(set(top_features))\n",
    "\n",
    "print(f\"Selected Features: {top_features}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d4f879-c7fd-42f7-a12a-22522b5d71a3",
   "metadata": {},
   "source": [
    "**Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with\n",
    "many features, including player statistics and team rankings. Explain how you would use the Embedded\n",
    "method to select the most relevant features for the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da640299-329a-4e58-a494-45a1b5faccf7",
   "metadata": {},
   "source": [
    "**ANSWER:------**\n",
    "\n",
    "To predict the outcome of a soccer match using an Embedded method for feature selection, you can leverage techniques that incorporate feature selection directly into the model training process. This ensures that the selected features are the most relevant for the predictive task. Here’s a step-by-step guide on how to do this:\n",
    "\n",
    "### Step-by-Step Process:\n",
    "\n",
    "1. **Understand the Data**:\n",
    "   - Familiarize yourself with the dataset, including player statistics (e.g., goals, assists, tackles), team rankings, and other relevant features.\n",
    "   - Identify the target variable, which in this case could be the match outcome (e.g., win, loss, draw).\n",
    "\n",
    "2. **Data Preprocessing**:\n",
    "   - **Handle Missing Values**: Impute or remove missing data.\n",
    "   - **Encode Categorical Variables**: Convert categorical variables (e.g., team names, positions) to numerical values.\n",
    "   - **Normalize/Scale Data**: Standardize numerical features to ensure they are on a similar scale.\n",
    "\n",
    "3. **Choose an Appropriate Model**:\n",
    "   - Select a model that supports embedded feature selection. Common choices include:\n",
    "     - **Lasso Regression** (L1 regularization): Encourages sparsity by penalizing the absolute values of the coefficients.\n",
    "     - **Tree-based Methods**: Models like Decision Trees, Random Forests, and Gradient Boosting Trees naturally rank features based on their importance.\n",
    "     - **Regularized Linear Models**: Elastic Net combines both L1 and L2 regularization.\n",
    "     - **Support Vector Machines (SVMs)** with regularization terms.\n",
    "\n",
    "4. **Train the Model with Embedded Feature Selection**:\n",
    "   - Train the chosen model on the dataset, allowing it to perform feature selection during the training process.\n",
    "   - For Lasso Regression, the regularization parameter (alpha) controls the degree of sparsity. A higher alpha leads to more feature coefficients being shrunk to zero.\n",
    "   - For tree-based methods, feature importance scores can be derived directly from the trained model.\n",
    "\n",
    "5. **Extract and Rank Features**:\n",
    "   - After training, extract the features selected by the model.\n",
    "   - For Lasso Regression, identify features with non-zero coefficients.\n",
    "   - For tree-based models, use feature importance scores to rank the features.\n",
    "\n",
    "6. **Validate and Fine-tune**:\n",
    "   - Evaluate the model’s performance using cross-validation.\n",
    "   - Adjust hyperparameters (e.g., regularization strength in Lasso, number of trees in Random Forest) to optimize performance.\n",
    "   - Ensure that the selected features consistently contribute to good model performance across different validation sets.\n",
    "\n",
    "7. **Iterate and Refine**:\n",
    "   - Based on model performance and feature importance, iterate on feature selection and model training.\n",
    "   - Consider domain knowledge to include or exclude certain features, ensuring that the model remains interpretable and relevant to the soccer match prediction task.\n",
    "\n",
    "\n",
    "### Key Points:\n",
    "- **Lasso Regression** shrinks coefficients of less important features to zero, effectively selecting a subset of relevant features.\n",
    "- **Random Forest** ranks features by importance, allowing selection based on their contribution to model accuracy.\n",
    "- **Cross-validation** ensures that the selected features generalize well to unseen data, avoiding overfitting.\n",
    "\n",
    "Using the Embedded method, you can leverage model-based feature selection techniques to identify and use the most relevant features for predicting soccer match outcomes, leading to more accurate and interpretable models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a49f96-078c-456c-8835-8912fe142c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features by Lasso Regression: Index([], dtype='object')\n",
      "Lasso Cross-Validation Scores: [-0.02604167 -0.11009462  0.00209124 -0.10943808 -0.02441406]\n",
      "Lasso Mean CV Score: -0.05357943697834031\n",
      "Selected Features by Random Forest: 3    Player2_Assists\n",
      "4       Team_Ranking\n",
      "0      Player1_Goals\n",
      "1    Player1_Assists\n",
      "2      Player2_Goals\n",
      "5     Home_Advantage\n",
      "Name: Feature, dtype: object\n",
      "Random Forest Cross-Validation Scores: [0.25   0.3125 0.5    0.5    0.375 ]\n",
      "Random Forest Mean CV Score: 0.3875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a sample dataset\n",
    "np.random.seed(42)\n",
    "data = pd.DataFrame({\n",
    "    'Player1_Goals': np.random.randint(0, 10, size=100),\n",
    "    'Player1_Assists': np.random.randint(0, 10, size=100),\n",
    "    'Player2_Goals': np.random.randint(0, 10, size=100),\n",
    "    'Player2_Assists': np.random.randint(0, 10, size=100),\n",
    "    'Team_Ranking': np.random.randint(1, 21, size=100),\n",
    "    'Home_Advantage': np.random.choice([0, 1], size=100),\n",
    "    'MatchOutcome': np.random.choice([0, 1], size=100)  # 0: Loss/Draw, 1: Win\n",
    "})\n",
    "\n",
    "# Preprocess data\n",
    "categorical_features = ['Home_Advantage']\n",
    "numerical_features = [col for col in data.columns if col not in categorical_features + ['MatchOutcome']]\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "data[numerical_features] = scaler.fit_transform(data[numerical_features])\n",
    "\n",
    "# Define features and target\n",
    "X = data.drop(columns=['MatchOutcome'])\n",
    "y = data['MatchOutcome']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Lasso Regression for Feature Selection ---\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Extract selected features\n",
    "selected_features_lasso = X_train.columns[lasso.coef_ != 0]\n",
    "print(f\"Selected Features by Lasso Regression: {selected_features_lasso}\")\n",
    "\n",
    "# Evaluate Lasso model\n",
    "scores_lasso = cross_val_score(lasso, X_train, y_train, cv=5)\n",
    "print(f\"Lasso Cross-Validation Scores: {scores_lasso}\")\n",
    "print(f\"Lasso Mean CV Score: {scores_lasso.mean()}\")\n",
    "\n",
    "# --- Random Forest for Feature Selection ---\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Extract feature importances\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Select top features\n",
    "selected_features_rf = feature_importances[feature_importances['Importance'] > 0.01]['Feature']\n",
    "print(f\"Selected Features by Random Forest: {selected_features_rf}\")\n",
    "\n",
    "# Evaluate Random Forest model\n",
    "scores_rf = cross_val_score(rf, X_train, y_train, cv=5)\n",
    "print(f\"Random Forest Cross-Validation Scores: {scores_rf}\")\n",
    "print(f\"Random Forest Mean CV Score: {scores_rf.mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb40716-93b4-45a2-bec2-50d7d2b64c32",
   "metadata": {},
   "source": [
    "**Q8. You are working on a project to predict the price of a house based on its features, such as size, location,\n",
    "and age. You have a limited number of features, and you want to ensure that you select the most important\n",
    "ones for the model. Explain how you would use the Wrapper method to select the best set of features for the\n",
    "predictor.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f0c95-0767-4f3f-a933-07e7ba875af6",
   "metadata": {},
   "source": [
    "**ANSWER:-----**\n",
    "\n",
    "Using the Wrapper method for feature selection involves evaluating different subsets of features based on model performance. This method is computationally intensive but can provide high-quality feature subsets by considering interactions between features. Here’s a step-by-step guide on how to use the Wrapper method to select the best set of features for predicting house prices.\n",
    "\n",
    "### Step-by-Step Process:\n",
    "\n",
    "1. **Understand the Data**:\n",
    "   - Familiarize yourself with the dataset, which includes features such as house size, location, age, and other relevant attributes.\n",
    "   - Identify the target variable, which in this case is the house price.\n",
    "\n",
    "2. **Data Preprocessing**:\n",
    "   - **Handle Missing Values**: Impute or remove missing data.\n",
    "   - **Encode Categorical Variables**: Convert categorical variables (e.g., location) to numerical values.\n",
    "   - **Normalize/Scale Data**: Standardize numerical features to ensure they are on a similar scale.\n",
    "\n",
    "3. **Choose a Model**:\n",
    "   - Select a machine learning model that you will use to evaluate feature subsets. Common choices include:\n",
    "     - Linear Regression\n",
    "     - Decision Trees\n",
    "     - Support Vector Machines (SVMs)\n",
    "     - Random Forests\n",
    "\n",
    "4. **Define a Search Strategy**:\n",
    "   - **Forward Selection**: Start with no features and add one feature at a time that improves model performance the most.\n",
    "   - **Backward Elimination**: Start with all features and remove one feature at a time that degrades model performance the least.\n",
    "   - **Recursive Feature Elimination (RFE)**: Train the model and remove the least important feature iteratively.\n",
    "\n",
    "5. **Evaluate Model Performance**:\n",
    "   - Use a performance metric appropriate for regression tasks, such as Mean Squared Error (MSE), Mean Absolute Error (MAE), or R-squared.\n",
    "   - Use cross-validation to ensure the performance is robust and not overfitted to the training data.\n",
    "\n",
    "6. **Implement the Wrapper Method**:\n",
    "   - Here, I will demonstrate using Recursive Feature Elimination (RFE) with a Linear Regression model.\n",
    "\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Sample Dataset Creation**:\n",
    "   - A sample dataset is created with features such as house size, location, age, bedrooms, bathrooms, and the target variable, price.\n",
    "\n",
    "2. **Data Preprocessing**:\n",
    "   - **Categorical Variables**: Convert categorical variables (e.g., 'Location') to numerical values using one-hot encoding.\n",
    "   - **Scaling**: Standardize numerical features to ensure they are on a similar scale.\n",
    "\n",
    "3. **Feature and Target Definition**:\n",
    "   - Split the data into features (`X`) and target (`y`).\n",
    "\n",
    "4. **Train-Test Split**:\n",
    "   - Split the data into training and test sets.\n",
    "\n",
    "5. **Recursive Feature Elimination (RFE)**:\n",
    "   - **Model Initialization**: Use Linear Regression as the base model.\n",
    "   - **RFE Initialization**: Initialize RFE to select the top 5 features.\n",
    "   - **RFE Fitting**: Fit RFE to the training data.\n",
    "   - **Feature Selection**: Extract and print the selected features.\n",
    "   - **Model Evaluation**: Evaluate the model using cross-validation and print the scores.\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "- **Wrapper Methods** are computationally expensive because they involve training and evaluating models multiple times for different subsets of features.\n",
    "- **RFE** is an iterative process that removes the least important feature based on model performance until the desired number of features is reached.\n",
    "- **Cross-validation** ensures that the feature selection is robust and generalizes well to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3792f74-4ed0-4641-b49f-789335508452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features by RFE: ['Size', 'Age', 'Location_B', 'Location_C', 'Location_D']\n",
      "RFE Cross-Validation Scores: [-1.12118143e+11 -6.37291349e+10 -7.59234649e+10 -6.08566543e+10\n",
      " -9.43256003e+10]\n",
      "RFE Mean CV Score: -81390599585.91771\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a sample dataset\n",
    "np.random.seed(42)\n",
    "data = pd.DataFrame({\n",
    "    'Size': np.random.randint(500, 3500, size=100),\n",
    "    'Location': np.random.choice(['A', 'B', 'C', 'D'], size=100),\n",
    "    'Age': np.random.randint(1, 100, size=100),\n",
    "    'Bedrooms': np.random.randint(1, 6, size=100),\n",
    "    'Bathrooms': np.random.randint(1, 4, size=100),\n",
    "    'Price': np.random.randint(100000, 1000000, size=100)\n",
    "})\n",
    "\n",
    "# Preprocess data\n",
    "data = pd.get_dummies(data, columns=['Location'], drop_first=True)\n",
    "\n",
    "# Define features and target\n",
    "X = data.drop(columns=['Price'])\n",
    "y = data['Price']\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Initialize RFE\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "\n",
    "# Fit RFE\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = [f for f, s in zip(data.drop(columns=['Price']).columns, rfe.support_) if s]\n",
    "print(f\"Selected Features by RFE: {selected_features}\")\n",
    "\n",
    "# Evaluate model with selected features\n",
    "scores = cross_val_score(model, X_train[:, rfe.support_], y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f\"RFE Cross-Validation Scores: {scores}\")\n",
    "print(f\"RFE Mean CV Score: {scores.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e5683-75db-4453-9065-513aea7dd76e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2694de5-1d8c-4e39-bb28-465f00449327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e3025-7cfc-4bdc-82a3-69cd0a39edb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
